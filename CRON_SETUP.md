# CRON SETUP - Automated Daily Scraping

**Installation Date:** 2026-01-30 18:25 UTC
**Status:** ‚úÖ ACTIVE

---

## üìã OVERVIEW

Automated daily scraping system that collects 500-1,000 Google Maps reviews every day at 3:00 UTC.

### Key Features
- **Smart chain rotation:** Prioritizes chains that haven't been scraped recently
- **Geographic diversity:** Selects 8 random cities from pool of 20 major US cities
- **Efficient resource usage:** ~30-60 min execution time, minimal API costs
- **Automatic log cleanup:** Removes logs older than 30 days

---

## üóìÔ∏è SCHEDULE

### Daily Scraper
- **Time:** 03:00 UTC (every day)
- **Duration:** 30-60 minutes
- **Target:** 500-1,000 new reviews
- **Chains:** 4 randomly selected (prioritized by last scrape time)
- **Cities:** 8 randomly selected from 20 major US cities

### Log Cleanup
- **Time:** 04:00 UTC (every Sunday)
- **Action:** Removes logs older than 30 days
- **Purpose:** Prevent disk space issues

---

## üìÅ FILES

### Scripts
```
/home/info_betsim/reviewsignal-5.0/scripts/
‚îú‚îÄ‚îÄ daily_scraper.py      - Main scraper logic (9.1 KB)
‚îú‚îÄ‚îÄ cron_wrapper.sh       - Cron execution wrapper (258 bytes)
‚îî‚îÄ‚îÄ (other scripts)
```

### Logs
```
/home/info_betsim/reviewsignal-5.0/logs/
‚îî‚îÄ‚îÄ daily_scraper.log     - Daily scraper output (rotated automatically)
```

### Configuration
```
Crontab: /var/spool/cron/crontabs/info_betsim
Cron service: systemd (cron.service)
```

---

## üîß CONFIGURATION DETAILS

### Crontab Entry
```bash
0 3 * * * /home/info_betsim/reviewsignal-5.0/scripts/cron_wrapper.sh >> /home/info_betsim/reviewsignal-5.0/logs/daily_scraper.log 2>&1
```

### Wrapper Script (`cron_wrapper.sh`)
```bash
#!/bin/bash
cd /home/info_betsim/reviewsignal-5.0 || exit 1
export GOOGLE_MAPS_API_KEY="AIzaSyDZYIYVfDYVV8KMtQdbKJEnYufhwswI3Wk"
/usr/bin/python3 scripts/daily_scraper.py
```

### Daily Scraper Logic (`daily_scraper.py`)

**Chain Selection Algorithm:**
1. Query database for last scrape timestamp per chain
2. Calculate priority: `days_since_last_scrape` (None = highest priority)
3. Sort chains by priority (descending)
4. Select top 4 chains

**Chains Pool (20 total):**
- Fast Food: Starbucks, McDonald's, KFC, Pizza Hut, Burger King, Subway, Domino's, Dunkin', Taco Bell, Chipotle
- Drugstores: CVS, Walgreens, Sephora
- Clothing: H&M, Zara, Gap, Old Navy
- Hotels: Marriott, Hilton, Holiday Inn

**Cities Pool (20 total):**
NYC, LA, Chicago, Houston, Phoenix, Philadelphia, San Antonio, San Diego, Dallas, San Jose, Austin, Jacksonville, San Francisco, Columbus, Indianapolis, Seattle, Denver, Boston, Miami, Atlanta

**Scraping Strategy:**
- 4 chains √ó 8 cities √ó 2-5 locations/city = 64-160 locations
- 64-160 locations √ó 5 reviews/location = 320-800 reviews (avg: ~600)

---

## üìä EXPECTED PERFORMANCE

### Daily Collection
| Metric | Min | Avg | Max |
|--------|-----|-----|-----|
| Reviews | 320 | 600 | 1000 |
| Locations | 64 | 120 | 160 |
| Chains | 4 | 4 | 4 |
| Cities | 8 | 8 | 8 |
| Duration | 20 min | 40 min | 60 min |
| API calls | 200 | 400 | 600 |

### Weekly Collection (7 days)
- **Reviews:** ~4,200 per week
- **Unique chains scraped:** All 20 chains (rotates every 5 days)
- **Geographic coverage:** Full coverage of 20 major US cities

### Monthly Collection (30 days)
- **Reviews:** ~18,000 per month
- **API Cost:** Within free tier ($200/month)
- **Data growth:** Continuous, fresh data every day

---

## üîç MONITORING

### Check Cron Status
```bash
# Verify cron service is running
sudo systemctl status cron

# View current crontab
crontab -l

# Check next scheduled run
grep -A 1 "DAILY REVIEW SCRAPER" <(crontab -l)
```

### Check Scraper Logs
```bash
# View today's log
tail -100 /home/info_betsim/reviewsignal-5.0/logs/daily_scraper.log

# Check last successful run
grep "DAILY SCRAPER COMPLETE" /home/info_betsim/reviewsignal-5.0/logs/daily_scraper.log | tail -1

# Count reviews collected today
sudo -u postgres psql -d reviewsignal -c "
  SELECT COUNT(*) as reviews_today
  FROM reviews
  WHERE source='google_maps'
  AND created_at::date = CURRENT_DATE;
"
```

### Check Database Stats
```bash
# Total reviews by source
sudo -u postgres psql -d reviewsignal -c "
  SELECT source, COUNT(*) as count
  FROM reviews
  GROUP BY source
  ORDER BY count DESC;
"

# Reviews collected in last 7 days
sudo -u postgres psql -d reviewsignal -c "
  SELECT DATE(created_at) as date, COUNT(*) as reviews
  FROM reviews
  WHERE source='google_maps'
  AND created_at > NOW() - INTERVAL '7 days'
  GROUP BY DATE(created_at)
  ORDER BY date DESC;
"
```

---

## üõ†Ô∏è MANUAL OPERATIONS

### Run Scraper Manually
```bash
# Run interactively (see output)
cd /home/info_betsim/reviewsignal-5.0
python3 scripts/daily_scraper.py

# Run in background (use for testing)
nohup python3 scripts/daily_scraper.py > logs/manual_run.log 2>&1 &
```

### Disable/Enable Cron Job
```bash
# Disable (comment out)
crontab -e
# Add '#' at start of scraper line

# Enable (remove comment)
crontab -e
# Remove '#' from scraper line

# Verify changes
crontab -l
```

### Force Immediate Run (Testing)
```bash
# This will run the scraper now (not at 3:00 UTC)
/home/info_betsim/reviewsignal-5.0/scripts/cron_wrapper.sh
```

---

## ‚ö†Ô∏è TROUBLESHOOTING

### Problem: Scraper not running
**Solution:**
```bash
# 1. Check cron service
sudo systemctl status cron
sudo systemctl restart cron

# 2. Check crontab syntax
crontab -l

# 3. Check script permissions
ls -l /home/info_betsim/reviewsignal-5.0/scripts/cron_wrapper.sh
chmod +x /home/info_betsim/reviewsignal-5.0/scripts/cron_wrapper.sh
```

### Problem: No reviews being saved
**Solution:**
```bash
# 1. Run manually to see errors
python3 /home/info_betsim/reviewsignal-5.0/scripts/daily_scraper.py

# 2. Check database connection
sudo -u postgres psql -d reviewsignal -c "SELECT 1;"

# 3. Check API key
echo $GOOGLE_MAPS_API_KEY
```

### Problem: Logs not appearing
**Solution:**
```bash
# 1. Check log directory exists
mkdir -p /home/info_betsim/reviewsignal-5.0/logs

# 2. Check permissions
chmod 755 /home/info_betsim/reviewsignal-5.0/logs

# 3. Check disk space
df -h /home
```

---

## üìà IMPACT ON SCALING ROADMAP

### Phase 1 (Weeks 1-4) - Data Collection
**Before cron setup:**
- Manual scraping required
- Inconsistent data collection
- Target: 50,000 reviews by Week 4

**After cron setup:**
- ‚úÖ Automatic daily collection (600 reviews/day)
- ‚úÖ Consistent data growth (4,200 reviews/week)
- ‚úÖ Expected Week 4 total: 30,000+ reviews (60% of target)

### Combined with Night Scraper
- Night scraper (one-time): 10,000 reviews
- Daily cron (4 weeks): 18,000 reviews
- **Total by Week 4:** 28,000 reviews
- **Additional manual runs:** 22,000 reviews needed
- **Strategy:** Run 3-4 more mass scraping sessions during Weeks 1-4

---

## üéØ SUCCESS METRICS

### Daily Metrics (Check every morning)
- [ ] Scraper ran successfully (check log)
- [ ] 500+ new reviews collected
- [ ] No errors in log file
- [ ] Database updated

### Weekly Metrics (Check every Monday)
- [ ] 4,000+ reviews collected this week
- [ ] All 20 chains scraped at least once
- [ ] Geographic diversity maintained
- [ ] API costs within budget

### Monthly Metrics
- [ ] 18,000+ reviews collected
- [ ] Data quality maintained (real Google Maps data)
- [ ] No downtime or failures
- [ ] Disk space healthy (logs rotating)

---

## üìù MAINTENANCE SCHEDULE

### Daily (Automated)
- ‚úÖ Scraper runs at 03:00 UTC
- ‚úÖ Logs appended to daily_scraper.log

### Weekly (Manual - 5 minutes)
- Check scraper ran successfully all 7 days
- Verify review count increased by ~4,000
- Scan logs for any errors or warnings

### Monthly (Manual - 15 minutes)
- Review total data collection (should be ~18,000/month)
- Check API usage (should be within free tier)
- Verify disk space (logs should auto-cleanup)
- Update documentation if needed

---

## üîÑ UPGRADE PATH

### Current: v1.0 (Basic Daily Scraping)
- 4 chains per day
- 8 cities per day
- 600 reviews per day

### Future: v2.0 (Smart Scaling)
- Increase to 6-8 chains per day
- Expand to 15 cities per day
- Target: 1,500 reviews per day
- Add adaptive rate limiting
- Add API cost monitoring

### Future: v3.0 (Enterprise)
- Multiple scraping windows (3:00, 9:00, 15:00, 21:00 UTC)
- Full coverage (all chains, all cities, daily)
- Target: 5,000+ reviews per day
- Real-time monitoring dashboard
- Automatic failure recovery

---

## ‚úÖ INSTALLATION VERIFICATION

**Date:** 2026-01-30 18:25 UTC
**Status:** COMPLETE

- [x] `daily_scraper.py` created (9.1 KB)
- [x] `cron_wrapper.sh` created (258 bytes)
- [x] Scripts made executable (chmod +x)
- [x] Logs directory created
- [x] Crontab installed
- [x] Cron service verified (running)
- [x] Manual test passed (scraper starts correctly)
- [x] Next run scheduled: 2026-01-31 03:00 UTC

---

**Next check:** 2026-01-31 08:00 UTC (morning after first run)
**Expected result:** 500-1,000 new reviews in database

---

*Generated: 2026-01-30 18:25 UTC*
*Cron Status: ‚úÖ ACTIVE*
*Next Run: 2026-01-31 03:00:00 UTC*
